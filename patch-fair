--- a/kernel/sched/fair.c	2012-04-13 18:55:19.047278657 +0200
+++ b/kernel/sched/fair.c	2012-05-22 19:43:02.984793614 +0200
@@ -335,6 +335,7 @@
 	se_depth = depth_se(*se);
 	pse_depth = depth_se(*pse);
 
+	//printk("se_deptg=%d, pse_depth=%d\n", se_depth, pse_depth);
 	while (se_depth > pse_depth) {
 		se_depth--;
 		*se = parent_entity(*se);
@@ -347,7 +348,9 @@
 
 	while (!is_same_group(*se, *pse)) {
 		*se = parent_entity(*se);
+		BUG_ON(!*se);
 		*pse = parent_entity(*pse);
+		BUG_ON(!*pse);
 	}
 }
 
@@ -370,14 +373,16 @@
 
 static inline struct cfs_rq *task_cfs_rq(struct task_struct *p)
 {
-	return &task_rq(p)->cfs;
+	//return &task_rq(p)->cfs;
+	return &(task_rq_fair_irmos(p)->cfs);
 }
 
 static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)
 {
 	struct task_struct *p = task_of(se);
-	struct rq *rq = task_rq(p);
-
+	//struct rq *rq = task_rq(p);
+	struct rq *rq = task_rq_fair_irmos(p);
+	/* This is equivalent to return se->cfs_rq; */
 	return &rq->cfs;
 }
 
@@ -679,6 +684,7 @@
 #if defined CONFIG_SMP && defined CONFIG_FAIR_GROUP_SCHED
 	cfs_rq->load_unacc_exec_time += delta_exec;
 #endif
+
 }
 
 static void update_curr(struct cfs_rq *cfs_rq)
@@ -686,10 +692,9 @@
 	struct sched_entity *curr = cfs_rq->curr;
 	u64 now = rq_of(cfs_rq)->clock_task;
 	unsigned long delta_exec;
-
 	if (unlikely(!curr))
 		return;
-
+	if( !is_irmos_task(task_of(curr))) {
 	/*
 	 * Get the amount of time the current task was running
 	 * since the last time we changed load (this cannot
@@ -711,6 +716,7 @@
 	}
 
 	account_cfs_rq_runtime(cfs_rq, delta_exec);
+	}
 }
 
 static inline void
@@ -1107,7 +1113,7 @@
 
 	update_stats_enqueue(cfs_rq, se);
 	check_spread(cfs_rq, se);
-	if (se != cfs_rq->curr)
+	if (se != cfs_rq->curr) 
 		__enqueue_entity(cfs_rq, se);
 	se->on_rq = 1;
 
@@ -2120,7 +2126,8 @@
 	struct sched_entity *se = &p->se;
 	struct cfs_rq *cfs_rq = cfs_rq_of(se);
 
-	WARN_ON(task_rq(p) != rq);
+	//WARN_ON(task_rq(p) != rq);
+	WARN_ON(task_rq_fair_irmos(p) != rq);
 
 	if (cfs_rq->nr_running > 1) {
 		u64 slice = sched_slice(cfs_rq, se);
@@ -2893,11 +2900,19 @@
  */
 static void check_preempt_wakeup(struct rq *rq, struct task_struct *p, int wake_flags)
 {
-	struct task_struct *curr = rq->curr;
-	struct sched_entity *se = &curr->se, *pse = &p->se;
-	struct cfs_rq *cfs_rq = task_cfs_rq(curr);
-	int scale = cfs_rq->nr_running >= sched_nr_latency;
-	int next_buddy_marked = 0;
+	struct task_struct *curr;
+	struct sched_entity *se, *pse;
+	struct cfs_rq *cfs_rq;
+	int scale;
+	int next_buddy_marked;
+
+	curr = rq->curr;
+	if( !curr)
+		WARN_ON(1);
+	se = &curr->se, pse = &p->se;
+	cfs_rq = task_cfs_rq(curr);
+	scale = cfs_rq->nr_running >= sched_nr_latency;
+	next_buddy_marked = 0;
 
 	if (unlikely(se == pse))
 		return;
@@ -2915,7 +2930,6 @@
 		set_next_buddy(pse);
 		next_buddy_marked = 1;
 	}
-
 	/*
 	 * We can come here with TIF_NEED_RESCHED already set from new task
 	 * wake up path.
@@ -5221,6 +5235,7 @@
 static void
 prio_changed_fair(struct rq *rq, struct task_struct *p, int oldprio)
 {
+	printk("entry : prio_changed_fair\n");
 	if (!p->se.on_rq)
 		return;
 
@@ -5423,6 +5438,7 @@
 			struct sched_entity *parent)
 {
 	struct rq *rq = cpu_rq(cpu);
+	//struct rq *rq = container_of(cfs_rq, struct rq, cfs);
 
 	cfs_rq->tg = tg;
 	cfs_rq->rq = rq;
