--- a/kernel/sched/irmos.c	1970-01-01 01:00:00.000000000 +0100
+++ g/kernel/sched/irmos.c	2012-05-30 13:49:54.245107674 +0200
@@ -0,0 +1,1393 @@
+#include "sched.h"
+#include <linux/ktime.h>
+#include <linux/slab.h>
+
+
+struct rt_bandwidth def_irmos_bandwidth;
+
+static enum hrtimer_restart
+sched_irmos_period_timer(struct hrtimer *hrtimer);
+
+void init_rq_irmos(struct rq *rq, struct rq *per_cpu)
+{
+        raw_spin_lock_init(&rq->lock);
+        rq->nr_running = 0;
+        rq->calc_load_active = 0;
+        rq->rt.rt_runtime = def_rt_bandwidth.rt_runtime;
+	rq->is_irmos = 1;
+        init_cfs_rq(&rq->cfs);
+        init_rt_rq(&rq->rt, rq);
+	rq->rt.rt_runtime = RUNTIME_INF;
+        atomic_set(&rq->nr_iowait, 0);
+#ifdef CONFIG_SMP
+	rq->cpu = per_cpu->cpu;
+#endif	
+
+#ifdef CONFIG_FAIR_GROUP_SCHED
+	INIT_LIST_HEAD(&rq->leaf_cfs_rq_list);
+#endif
+#ifdef CONFIG_RT_GROUP_SCHED
+	INIT_LIST_HEAD(&rq->leaf_rt_rq_list);
+#endif
+}
+
+static void init_irmos_rq(struct irmos_rq *irmos_rq, struct rq *rq)
+{
+	struct irmos_prio_array *array;
+	int i;
+
+	array = &irmos_rq->active;
+	for( i = 0; i < MAX_PRIO; i++) {
+                array->task_prio[i] = 0;
+                __clear_bit(i, array->bitmap);
+        }
+
+        __set_bit(MAX_PRIO, array->bitmap);
+
+        irmos_rq->highest_prio = MAX_PRIO;
+
+	irmos_rq->irmos_time = 0;
+        irmos_rq->irmos_throttled = 0;
+        irmos_rq->irmos_deadline = 0;
+        irmos_rq->irmos_runtime = 0; 
+        irmos_rq->irmos_period = def_irmos_bandwidth.rt_period;
+	irmos_rq->irmos_start_time = 0;
+
+        raw_spin_lock_init(&irmos_rq->irmos_runtime_lock);
+
+        hrtimer_init(&irmos_rq->irmos_period_timer, CLOCK_MONOTONIC,
+							HRTIMER_MODE_REL);
+        irmos_rq->irmos_period_timer.function = sched_irmos_period_timer;
+
+	irmos_rq->irmos_nr_boosted = 0;
+        irmos_rq->rq = rq;
+        init_rq_irmos(&irmos_rq->rq_, rq);
+        irmos_rq->irmos_needs_resync = false;
+	RB_CLEAR_NODE(&irmos_rq->rb_node);
+}
+
+void init_irmos_edf_tree(struct irmos_edf_tree *tree) {
+	tree->rb_root = RB_ROOT;
+	tree->rb_leftmost = NULL;
+}
+
+struct irmos_rq *create_irmos_rq(struct rq *rq)
+{
+	struct irmos_rq *irmos_rq;
+	
+	irmos_rq = kzalloc(sizeof(struct irmos_rq), GFP_KERNEL);
+	init_irmos_rq(irmos_rq, rq);
+
+	return irmos_rq;
+}
+
+struct hyper_irmos_rq * create_hyper_irmos_rq(cpumask_var_t cpus_allowed)
+{
+	int i;
+	struct irmos_rq *irmos_rq;
+	struct hyper_irmos_rq *hir = kzalloc(sizeof(struct hyper_irmos_rq), 
+								GFP_KERNEL);
+	if( !hir)
+		goto hir_creation_err;
+	
+	if(!alloc_cpumask_var(&hir->cpus_allowed, GFP_KERNEL))
+		goto hir_creation_err;
+
+	hir->irmos_rq = kzalloc(sizeof(irmos_rq) * nr_cpu_ids, GFP_KERNEL);
+	if( !hir->irmos_rq)
+		goto hir_creation_err;
+
+	cpumask_copy(hir->cpus_allowed, cpus_allowed);
+	for_each_cpu(i, cpus_allowed) 
+		hir->irmos_rq[i] = create_irmos_rq(cpu_rq(i));
+	
+	return hir;
+hir_creation_err:
+	return NULL;
+}
+	
+struct hyper_irmos_rq* 
+create_empty_hyper_irmos_rq(cpumask_var_t cpus_allowed)
+{
+	int i;
+        struct irmos_rq *irmos_rq;
+        struct hyper_irmos_rq *hir = kzalloc(sizeof(struct hyper_irmos_rq),
+                                                                GFP_KERNEL);
+        if( !hir)
+                goto empty_hir_creation_err;
+
+        if(!alloc_cpumask_var(&hir->cpus_allowed, GFP_KERNEL))
+                goto empty_hir_creation_err;
+
+        hir->irmos_rq = kzalloc(sizeof(irmos_rq) * nr_cpu_ids, GFP_KERNEL);
+        if( !hir->irmos_rq)
+                goto empty_hir_creation_err;
+
+        cpumask_copy(hir->cpus_allowed, cpus_allowed);
+        for_each_cpu(i, cpus_allowed)
+                hir->irmos_rq[i] = NULL;
+
+	return hir;
+empty_hir_creation_err:
+	return NULL;
+}
+
+int attach_irmos_rq_to_hir(struct hyper_irmos_rq *hir, struct irmos_rq *ir)
+{
+	int cpu;
+#ifdef CONFIG_SMP
+	cpu = ir->rq->cpu;
+#else
+	cpu = 0;
+#endif
+	cpumask_set_cpu(cpu, hir->cpus_allowed);
+	hir->irmos_rq[cpu] = ir;
+
+	return 1;
+}
+
+static void start_irmos_period_timer(struct irmos_rq *irmos_rq);
+void init_irmos_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime)
+{
+	rt_b->rt_period = ns_to_ktime(period);
+        rt_b->rt_runtime = runtime;
+
+        raw_spin_lock_init(&rt_b->rt_runtime_lock);
+}
+
+void set_irmos_rq_bandwidth(struct irmos_rq *irmos_rq, u64 period, u64 runtime)
+{
+	raw_spin_lock(&irmos_rq->irmos_runtime_lock);
+
+	irmos_rq->irmos_period = ns_to_ktime(period);
+        irmos_rq->irmos_runtime = runtime;
+	irmos_rq->irmos_bw = to_ratio(period, runtime);
+
+        raw_spin_unlock(&irmos_rq->irmos_runtime_lock);
+}
+
+#ifdef CONFIG_FAIR_GROUP_SCHED
+static inline void list_del_leaf_cfs_rq(struct cfs_rq *cfs_rq)
+{
+        if (cfs_rq->on_list) {
+                list_del_rcu(&cfs_rq->leaf_cfs_rq_list);
+                cfs_rq->on_list = 0;
+        }
+}
+
+static void init_tg_cfs_entry_irmos(struct task_group *tg, 
+				struct cfs_rq *cfs_rq,
+				struct sched_entity *se, int cpu,
+				struct sched_entity *parent,
+				struct irmos_rq *irmos_rq)
+{
+	init_tg_cfs_entry(tg, cfs_rq, se, cpu, parent);
+	//cfs_rq->rq = &irmos_rq->rq_;
+	//list_del_leaf_cfs_rq(tg->cfs_rq[cpu]);
+	tg->cfs_rq[cpu]->rq = &irmos_rq->rq_;
+	if( !parent && se)
+		se->cfs_rq = &irmos_rq->rq_.cfs;
+}
+#endif
+
+#ifdef CONFIG_RT_GROUP_SCHED
+static void init_tg_rt_entry_irmos(struct task_group *tg, 
+				struct rt_rq *rt_rq,
+				struct sched_rt_entity *rt_se, int cpu,
+				struct sched_rt_entity *parent,
+				struct irmos_rq *irmos_rq)
+{
+	
+	init_tg_rt_entry(tg, rt_rq, rt_se, cpu, parent);
+	rt_rq->rq = &irmos_rq->rq_;
+	tg->rt_rq[cpu]->rq = &irmos_rq->rq_;
+	if( !parent && !rt_se) {
+		tg->rt_rq[cpu]->rt_runtime = tg->rt_bandwidth.rt_runtime;
+	}
+		
+	if( !parent && rt_se)
+		rt_se->rt_rq = &irmos_rq->rq_.rt;
+}
+#endif
+
+static void before_redirect_rq(struct task_struct *tsk, 
+					struct cgroup_scanner *scan)
+{
+	int *cpu = scan->data;
+#ifdef CONFIG_SMP
+        if( task_thread_info(tsk)->cpu != *cpu)
+                return;
+#endif
+	if( is_irmos_task(tsk))
+		irmos_sched_class.dequeue_task(cpu_rq(*cpu), tsk, 0);
+	else 
+		tsk->sched_class->dequeue_task(cpu_rq(*cpu), tsk, 0);
+}
+
+/*
+ * This is where the "ghost task" bug is from !!!
+ */
+static void redirect_rq(struct task_struct *tsk, struct cgroup_scanner *scan)
+{
+	int *cpu = scan->data;
+#ifdef CONFIG_SMP
+	if( task_thread_info(tsk)->cpu != *cpu)
+		return;
+#endif
+	//tsk->sched_class->dequeue_task(cpu_rq(*cpu), tsk, 0);
+/*	if( is_irmos_task(tsk))
+		irmos_sched_class.dequeue_task(cpu_rq(*cpu), tsk, 0);
+	else 
+		tsk->sched_class->dequeue_task(cpu_rq(*cpu), tsk, 0);
+*/	set_task_rq(tsk, *cpu); 
+	irmos_sched_class.enqueue_task(cpu_rq(*cpu), tsk, 0);
+}
+
+/*
+ * TODO : the ** irmos_rq inside task_group is not considered here
+ */
+void attach_task_group_to_hyper_irmos_rq(struct hyper_irmos_rq *hir,
+						struct task_group *tg)
+{
+	int i;
+	struct task_group *child, *parent;
+	struct cgroup_scanner scan;
+
+	local_irq_disable();
+/*
+	tg->parent = NULL;
+	tg->irmos_label = 1;
+	tg->hir = hir;
+*/
+	/* Firstly , dequeue tasks inside this cgroup and its sub cgroups  */
+	for_each_possible_cpu(i) {
+		/* dequeue tasks inside this cgroup  */
+		scan.cg = tg->css.cgroup;
+		scan.test_task = NULL;
+		scan.process_task = before_redirect_rq;
+		scan.heap = NULL;
+		scan.data = &i;
+
+		cgroup_scan_tasks(&scan);
+		
+		/* dequeue tasks in sub cgroups  */
+		parent = tg;
+step1:
+		list_for_each_entry_rcu(child, &parent->children, siblings) {
+			
+			if( child->hir)			
+				continue;
+
+			scan.process_task = before_redirect_rq;
+			scan.cg = child->css.cgroup;
+			cgroup_scan_tasks(&scan);
+
+			parent = child;
+			goto step1;
+step2:		
+			continue;
+		}
+
+		if( parent == tg)
+			goto step3;
+	
+		child = parent;
+		parent = parent->parent;
+		goto step2;
+step3:
+		continue;
+	}
+
+	tg->parent = NULL;
+	tg->irmos_label = 1;
+	tg->hir = hir;
+
+ 	for_each_possible_cpu(i) {
+
+#ifdef CONFIG_FAIR_GROUP_SCHED
+               /* hir->irmos_rq[i]->rq_.cfs.tg = tg;
+                hir->irmos_rq[i]->rq_.cfs.rq = &hir->irmos_rq[i]->rq_;
+                tg->cfs_rq[i] = &hir->irmos_rq[i]->rq_.cfs;
+                tg->se[i] = NULL;
+		*/
+		init_tg_cfs_entry_irmos(tg, &hir->irmos_rq[i]->rq_.cfs,
+						NULL, i, NULL,
+						hir->irmos_rq[i]);
+#endif
+#ifdef CONFIG_RT_GROUP_SCHED
+            /*    hir->irmos_rq[i]->rq_.rt.highest_prio.curr = MAX_RT_PRIO;
+                hir->irmos_rq[i]->rq_.rt.rt_nr_boosted = 0;
+                hir->irmos_rq[i]->rq_.rt.rq = &hir->irmos_rq[i]->rq_;
+                hir->irmos_rq[i]->rq_.rt.tg = tg;
+
+                tg->rt_rq[i] = &hir->irmos_rq[i]->rq_.rt;
+                tg->rt_se[i] = NULL;
+	    */
+		init_tg_rt_entry_irmos(tg, &hir->irmos_rq[i]->rq_.rt,
+						NULL, i, NULL,
+						hir->irmos_rq[i]);
+#endif
+		/* redirect tasks', belonging to  this cgroup, runqueue 
+		 * and enqueue them in the new runqueues 
+		 */
+                scan.cg = tg->css.cgroup;
+                scan.test_task = NULL;
+                scan.process_task = redirect_rq;
+                scan.heap = NULL;
+                scan.data = &i;
+                cgroup_scan_tasks(&scan);
+
+		parent = tg;
+re_q1:
+                list_for_each_entry_rcu(child, &parent->children, siblings) {
+
+			if( child->hir)			
+				continue;
+#ifdef CONFIG_FAIR_GROUP_SCHED
+                        init_tg_cfs_entry_irmos(child, child->cfs_rq[i],
+                                        child->se[i], i, child->parent->se[i],
+                                        hir->irmos_rq[i]);
+#endif
+
+#ifdef CONFIG_RT_GROUP_SCHED
+                        init_tg_rt_entry_irmos(child, child->rt_rq[i],
+                                        child->rt_se[i], i,
+                                        child->parent->rt_se[i],
+                                        hir->irmos_rq[i]);
+#endif
+
+                        child->hir = hir;
+                        child->irmos_label = 100;
+
+                        scan.cg = child->css.cgroup;
+			scan.process_task = redirect_rq;
+			scan.data = &i;
+                        cgroup_scan_tasks(&scan);
+                        parent = child;
+                        goto re_q1;
+re_q2:
+                        continue;
+                }
+
+                if( parent == tg)
+                        goto re_q3;
+
+                child = parent;
+                parent = parent->parent;
+                goto re_q2;
+re_q3:
+
+                continue;
+        }
+
+
+	local_irq_enable();
+}
+
+/* Attach all tasks (cfs, rt or even dl) to a hyper_irmos_rq */
+void attach_cgroup_to_hyper_irmos_rq(struct hyper_irmos_rq *hir, 
+							struct cgroup *cgrp)
+{
+	struct task_group *tg = 
+		container_of(cgroup_subsys_state(cgrp, cpu_cgroup_subsys_id),
+							struct task_group, css);
+	attach_task_group_to_hyper_irmos_rq(hir, tg);
+}
+
+int alloc_irmos_sched_group(struct task_group *tg, struct task_group *parent)
+{
+	int i;
+
+	tg->hir = parent->hir;
+
+	if( parent->hir) {
+		for_each_possible_cpu(i) {
+#ifdef CONFIG_FAIR_GROUP_SCHED
+			tg->cfs_rq[i]->rq = 
+				&parent->hir->irmos_rq[i]->rq_;
+			if( !parent->se[i] && tg->se[i])
+				tg->se[i]->cfs_rq = 
+					&parent->hir->irmos_rq[i]->rq_.cfs;
+#endif
+#ifdef CONFIG_RT_GROUP_SCHED
+			tg->rt_rq[i]->rq =
+				&parent->hir->irmos_rq[i]->rq_;
+			if( !parent->se[i] && tg->se[i])
+				tg->rt_se[i]->rt_rq =
+					&parent->hir->irmos_rq[i]->rq_.rt;
+#endif
+		}
+		tg->irmos_label = 100;
+	}
+	else 
+		tg->irmos_label = 0;
+	
+	return 1;
+}
+
+int is_irmos_task(struct task_struct *p)
+{
+	struct rq *rq;
+
+	if(!rt_task(p)) {
+#ifdef CONFIG_FAIR_GROUP_SCHED
+		rq = p->se.cfs_rq->rq;
+#else
+		rq = task_rq_fair_irmos(p);
+#endif
+	}
+	else {
+#ifdef CONFIG_RT_GROUP_SCHED
+		rq = p->rt.rt_rq->rq;
+#else
+		rq = task_rq_rt_irmos(p);
+#endif
+	}
+
+	return rq->is_irmos == 1;
+}
+
+struct irmos_rq*
+irmos_rq_of_task(struct task_struct *p)
+{
+	struct task_group *tg;
+	unsigned int cpu;
+	struct irmos_rq *irmos_rq;
+	struct rq *rq_;
+
+	if( !is_irmos_task(p))
+		return NULL;
+
+	tg = task_group(p);
+	cpu = task_cpu(p);
+	if( !rt_task(p)) { 
+#ifdef CONFIG_FAIR_GROUP_SCHED
+		//rq_ = container_of(tg->cfs_rq[cpu], struct rq, cfs); 
+		rq_ = p->se.cfs_rq->rq;
+#else
+		rq_ = task_rq_fair_irmos(p);
+#endif
+	}
+	else {
+#ifdef CONFIG_RT_GROUP_SCHED
+		//rq_ = container_of(tg->rt_rq[cpu], struct rq, rt);
+		rq_ = p->rt.rt_rq->rq;
+#else
+		rq_ = task_rq_rt_irmos(p);
+#endif
+	}
+		
+	irmos_rq = container_of(rq_, struct irmos_rq, rq_);
+	
+	return irmos_rq;
+}
+
+static int
+task_is_boosted(struct task_struct *p)
+{
+	return p->prio != p->normal_prio;
+}
+
+static inline int
+irmos_rq_throttled(struct irmos_rq *irmos_rq)
+{	
+	return irmos_rq->irmos_throttled && !irmos_rq->irmos_nr_boosted;
+}
+
+static int
+irmos_rq_boosted(struct irmos_rq *irmos_rq)
+{
+	if( !irmos_rq->irmos_nr_boosted)
+		return MAX_PRIO;
+	return irmos_rq->highest_prio;
+}
+
+/*
+ * Return the local runqueue of a irmos_rq
+ */
+static inline struct rq*
+rq_of_irmos_rq(struct irmos_rq *irmos_rq)
+{
+	return &irmos_rq->rq_;
+}
+
+/*
+ * irmos_rq' are stored in a rb-tree ordered
+ * by their deadline.
+ */
+static inline int
+irmos_rq_on_rq(struct irmos_rq *irmos_rq)
+{
+	return !RB_EMPTY_NODE(&irmos_rq->rb_node);
+}
+
+/*
+ * Simply compare the number a and b
+ */
+static inline int
+irmos_time_before(u64 a, u64 b)
+{
+	return (s64)(a-b) < 0;
+}
+
+/*
+ * return the highest priority of tasks inside the irmos_rq.
+ */
+static inline int
+irmos_rq_prio(struct irmos_rq *irmos_rq)
+{
+	return irmos_rq->highest_prio;
+}
+
+/*
+ * conpare two irmos_rqs' priority
+ */  
+static inline int
+irmos_rq_before(struct irmos_rq *a, struct irmos_rq *b)
+{
+	/*
+	 * Schedule by priority if :
+	 * - both a and b are boosted
+	 * - throttling is disabled system-wide
+	 */
+#if 0
+	printk("a->irmos_runtime=%llu, b->irmos_runtime=%llu\n", 
+					a->irmos_runtime, b->irmos_runtime);
+	printk("a->prio=%d, b->prio=%d\n", a->highest_prio, b->highest_prio);
+	printk("a->irmos_nr_boosted=%d, b->irmos_nr_boosted=%d\n",
+				a->irmos_nr_boosted, b->irmos_nr_boosted);
+	printk("global_rt_runtime = %llu\n", global_rt_runtime());
+	printk("a->deadline - b->deadline=%lld\n", 
+				(s64)(a->irmos_deadline - b->irmos_deadline));
+	printk("======================================================\n");
+#endif
+	if( (a->irmos_nr_boosted && b->irmos_nr_boosted) ||
+		global_rt_runtime() == RUNTIME_INF)
+			return irmos_rq_prio(a) < irmos_rq_prio(b);
+	
+	/* Only a is boosted, choose it. */
+	if( a->irmos_nr_boosted)
+		return 1;
+	/* Only b is boosted, choose it. */
+	if( b->irmos_nr_boosted)
+		return 0;
+	/* Order by deadline */
+	return irmos_time_before(a->irmos_deadline, b->irmos_deadline);
+}
+
+static void
+irmos_rq_update_deadline(struct irmos_rq *irmos_rq)
+{
+	/* maybe it's better to use &irmos_rq->rq_ here */
+	struct rq *rq = irmos_rq->rq;
+	u64 runtime, period, left, right;
+
+	printk("irmos_rq_update_deadline \n");
+
+	raw_spin_lock(&irmos_rq->irmos_runtime_lock);
+
+	//irmos_rq->irmos_start_time = rq->clock_task;
+	runtime = irmos_rq->irmos_runtime;
+	period = ktime_to_ns(irmos_rq->irmos_period);
+
+	/*
+	 * Update the deadline if
+	 * - it is in the past
+	 * - using it would lead to a timeframe during which the
+	 *   group would exceed i-st allocated bandwidth
+	 */
+	if( irmos_time_before(irmos_rq->irmos_deadline, rq->clock))
+		goto update;
+
+	WARN_ON_ONCE(irmos_rq->irmos_time > runtime);
+
+	left = period * (runtime - irmos_rq->irmos_time);
+	right = (irmos_rq->irmos_deadline - rq->clock)*irmos_rq->irmos_runtime;
+
+
+	if( left > right) {
+update:
+		irmos_rq->irmos_deadline = rq->clock + period;
+		irmos_rq->irmos_time -= min(runtime, irmos_rq->irmos_time);
+
+		while( irmos_rq->irmos_time > runtime) {
+			irmos_rq->irmos_deadline += period;
+			irmos_rq->irmos_time -= runtime;
+		}
+		
+		
+		/* 
+		 * In my opinion, it's better to do a further check:
+		 */
+/*		left = period * (runtime - irmos_rq->irmos_time);
+		right = (irmos_rq->deadline - rq->clock)*
+						irmos_rq->irmos_runtime;
+		if( left > right) {
+			irmos_rq->irmos_deadline = rq->clock + period;
+			irmos_rq->irmos_time -=  irmos_rq->irmos_time;
+		}
+*/		
+		if( hrtimer_active(&irmos_rq->irmos_period_timer))
+			irmos_rq->irmos_needs_resync = true;
+	}
+		
+	raw_spin_unlock(&irmos_rq->irmos_runtime_lock);
+}
+		
+static void
+__enqueue_irmos_rq(struct irmos_rq *irmos_rq)
+{
+	/*
+	 * The per-cpu runqueue this irmos_rq points to.
+	 */
+	struct rq *rq = irmos_rq->rq;
+	/*
+	 * The rb-tree used to store irmos_rq'
+ 	 */
+	struct irmos_edf_tree *irmos_tree = &rq->irmos; 
+	struct rb_node **link = &(irmos_tree->rb_root.rb_node);
+	struct rb_node *parent = NULL;
+	struct irmos_rq *entry;
+	int leftmost = 1;
+
+	BUG_ON(irmos_rq_on_rq(irmos_rq));
+
+	while(*link) {
+		parent = *link;
+		entry = rb_entry(parent, struct irmos_rq, rb_node);
+		
+		if( irmos_rq_before(irmos_rq, entry))
+			link = &parent->rb_left;
+		else {
+			link = &parent->rb_right;
+			leftmost = 0;
+		}
+	}
+
+	if( leftmost){
+		irmos_tree->rb_leftmost = &irmos_rq->rb_node;
+		/* Will this be the solution for the biggest bug? */
+		//irmos_rq->irmos_start_time = rq->clock_task;
+	}
+
+	rb_link_node(&irmos_rq->rb_node, parent, link);
+	rb_insert_color(&irmos_rq->rb_node, &irmos_tree->rb_root);
+}
+
+static void __dequeue_irmos_rq(struct irmos_rq *irmos_rq);
+
+static void
+enqueue_irmos_rq(struct irmos_rq *irmos_rq, int old_boosted)
+{
+	int on_rq = irmos_rq_on_rq(irmos_rq);
+	BUG_ON(!irmos_rq->irmos_nr_running);
+	BUG_ON(on_rq && irmos_rq_throttled(irmos_rq));
+
+	if(on_rq) {
+		if( old_boosted != irmos_rq_boosted(irmos_rq)) 
+			__dequeue_irmos_rq(irmos_rq);
+		else 
+		{	
+			/* Already queued properly. */
+			return;
+		}
+	}
+
+	if( irmos_rq_throttled(irmos_rq))
+		return;
+
+	if( !irmos_rq->irmos_nr_boosted) {
+		irmos_rq_update_deadline(irmos_rq);
+		on_rq = irmos_rq_on_rq(irmos_rq);
+	}
+
+	if( !on_rq) {
+		__enqueue_irmos_rq(irmos_rq);
+	}
+}
+
+static void update_curr_irmos(struct rq *rq);
+static void
+__dequeue_irmos_rq(struct irmos_rq *irmos_rq)
+{
+	struct rq *rq = irmos_rq->rq;
+	struct irmos_edf_tree *irmos_tree = &rq->irmos;
+	BUG_ON(!irmos_rq_on_rq(irmos_rq));
+	
+	/* Will this be the solution for the biggest bug? */
+	update_curr_irmos(irmos_rq->rq);
+
+	if( irmos_tree->rb_leftmost == &irmos_rq->rb_node) {
+		irmos_tree->rb_leftmost = rb_next(&irmos_rq->rb_node);
+	}
+
+	rb_erase(&irmos_rq->rb_node, &irmos_tree->rb_root);
+	RB_CLEAR_NODE(&irmos_rq->rb_node);
+}
+
+static void
+dequeue_irmos_rq(struct irmos_rq *irmos_rq, int old_boosted)
+{
+	int on_rq = irmos_rq_on_rq(irmos_rq);
+	/*
+	 * Here we do not expect throttled irmos-rq to be in the EDF
+	 * tree; note that when they exceeded their assigned budget,
+	 * they are dequeued via sched_irmos_rq_dequeue().
+	 */
+	BUG_ON(on_rq && irmos_rq_throttled(irmos_rq));
+
+	/* Will this be the solution for the biggest bug? */
+//	update_curr_irmos(irmos_rq->rq);
+
+	if( on_rq && (!irmos_rq->irmos_nr_running ||
+			old_boosted != irmos_rq_boosted(irmos_rq))) {
+		/*
+		 * dequeue the irmos_rq if it has no more tasks or
+		 * its boosted priority changed.
+		 */
+		__dequeue_irmos_rq(irmos_rq);
+		on_rq = 0;
+	}
+
+	/*
+	 * If we do not need to requeue the irmos_rq, just return.
+	 */
+	if( !irmos_rq->irmos_nr_running || irmos_rq_throttled(irmos_rq)) {
+		return;
+	}
+
+	if(!on_rq)
+		__enqueue_irmos_rq(irmos_rq);
+}
+
+static void
+inc_irmos_group(struct task_struct *p, struct irmos_rq *irmos_rq)
+{
+	if( task_is_boosted(p))
+		irmos_rq->irmos_nr_boosted ++;
+}
+
+static void
+dec_irmos_group(struct task_struct *p, struct irmos_rq *irmos_rq)
+{
+	if( task_is_boosted(p))
+		irmos_rq->irmos_nr_boosted --;
+}
+
+static void
+inc_irmos_prio(struct irmos_rq *irmos_rq, int prio)
+{
+	int prev_prio = irmos_rq->highest_prio;
+	if( prio < prev_prio)
+		irmos_rq->highest_prio = prio;
+}
+
+static void
+dec_irmos_prio(struct irmos_rq *irmos_rq, int prio)
+{
+	int prev_prio = irmos_rq->highest_prio;
+	struct irmos_prio_array *array;
+	
+	if(irmos_rq->irmos_nr_running) {
+		if( prio == prev_prio) {
+			array = &irmos_rq->active;
+			irmos_rq->highest_prio = 
+				sched_find_first_bit(array->bitmap);
+		}
+	}
+	else
+		irmos_rq->highest_prio = MAX_PRIO;
+}
+
+static inline void
+inc_irmos_tasks(struct task_struct *p, struct irmos_rq *irmos_rq)
+{
+	int prio = p->prio;
+	struct irmos_prio_array *array = &irmos_rq->active;
+
+	if( array->task_prio[prio] ++ == 0)
+		__set_bit(p->prio, array->bitmap);
+
+	irmos_rq->irmos_nr_running ++;
+
+	inc_irmos_prio(irmos_rq, prio);
+	inc_irmos_group(p, irmos_rq);
+}
+
+static inline void
+dec_irmos_tasks(struct task_struct *p, struct irmos_rq *irmos_rq)
+{
+	int prio = p->prio;
+	struct irmos_prio_array *array = &irmos_rq->active;
+
+	if( -- array->task_prio[prio] == 0)
+		__clear_bit(p->prio, array->bitmap);
+
+	irmos_rq->irmos_nr_running --;
+
+	dec_irmos_prio(irmos_rq, prio);
+	dec_irmos_group(p, irmos_rq);
+}
+
+static u64
+sched_irmos_runtime(struct irmos_rq *irmos_rq)
+{
+	return irmos_rq->irmos_runtime;
+}
+
+/*
+ * The left most irmos_rq on the edf tree has latested deadline
+ */
+static inline bool
+irmos_rq_is_leftmost(struct irmos_rq *irmos_rq)
+{
+	struct rq *rq = irmos_rq->rq; //rq_of_irmos_rq(irmos_rq);
+	
+	return rq->irmos.rb_leftmost == &irmos_rq->rb_node;
+}
+
+static void
+sched_irmos_rq_dequeue(struct irmos_rq *irmos_rq)
+{
+	if( irmos_rq_on_rq(irmos_rq))
+		__dequeue_irmos_rq(irmos_rq);
+}
+
+static void
+sched_irmos_rq_enqueue(struct irmos_rq *irmos_rq)
+{
+	if( irmos_rq->irmos_nr_running && !irmos_rq_on_rq(irmos_rq)) {
+		__enqueue_irmos_rq(irmos_rq);
+		if( irmos_rq_is_leftmost(irmos_rq))
+			resched_task(irmos_rq->rq->curr);
+	}
+}
+
+
+static void start_irmos_period_timer(struct irmos_rq *irmos_rq);
+
+/*
+ * This method is not complete yet ...
+ */
+static int
+sched_irmos_runtime_exceeded(struct irmos_rq *irmos_rq)
+{
+	u64 runtime = sched_irmos_runtime(irmos_rq);
+
+	if( irmos_rq->irmos_throttled)
+		return irmos_rq_throttled(irmos_rq);
+
+	if( runtime == RUNTIME_INF)
+		return 0;
+
+	if( runtime >= ktime_to_ns(irmos_rq->irmos_period))
+		return 0;
+
+	if( irmos_rq->irmos_time < runtime)
+		return 0;
+
+	if( irmos_rq->irmos_time >= irmos_rq->irmos_runtime) {
+
+		if( irmos_rq->irmos_runtime) {
+			irmos_rq->irmos_throttled = 1;
+			start_irmos_period_timer(irmos_rq);
+		}
+		else {
+			irmos_rq->irmos_time = 0;
+			printk_once("sched: RT throttling bypassed\n");
+		}
+	
+		if( irmos_rq_throttled(irmos_rq)) {
+			sched_irmos_rq_dequeue(irmos_rq);
+			return 1;
+		}
+	}
+	
+	return 0;
+}
+
+
+static void
+update_curr_irmos(struct rq *rq)
+{
+	struct task_struct *curr = rq->curr;
+	struct irmos_rq *irmos_rq = irmos_rq_of_task(curr);
+	u64 delta_exec;
+
+	if( !irmos_rq)
+		return;
+
+	/* This is for bandwidth reservation of the irmos_rq */
+	delta_exec = rq->clock_task - irmos_rq->irmos_start_time;
+	irmos_rq->irmos_start_time = rq->clock_task;
+	if( unlikely((s64)delta_exec < 0))
+		delta_exec = 0; 
+
+	raw_spin_lock(&irmos_rq->irmos_runtime_lock);
+
+	irmos_rq->irmos_time += delta_exec;
+	if(sched_irmos_runtime_exceeded(irmos_rq))
+	{
+		//printk("irmos_time=%llu, irmos_runtime=%llu\n",
+		//		irmos_rq->irmos_time, irmos_rq->irmos_runtime);
+		resched_task(curr);
+	}
+	raw_spin_unlock(&irmos_rq->irmos_runtime_lock);
+}
+
+/*
+ * Maybe this is the reason for the "ghost task" bug ...
+ * no. it shoud not be, yet this is a bug
+ */
+static inline int
+check_preempt_irmos_rq(struct task_struct *curr, 
+				struct task_struct *p, int flags)
+{
+	struct irmos_rq *irmos_rq = irmos_rq_of_task(p);
+	struct irmos_rq *irmos_rq_curr = irmos_rq_of_task(curr);
+	const struct sched_class *class;
+
+	printk("check_preempt_irmos_rq\n");
+
+/*	if(irmos_rq_throttled(irmos_rq))
+	{	
+		return 0;
+	}
+*/
+	if( !irmos_rq_curr)
+		return 1;
+
+	if( irmos_rq_curr == irmos_rq) {
+		//p->sched_class->check_preempt_curr(&irmos_rq->rq_, p, flags); 
+		if (p->sched_class == curr->sched_class) {
+			curr->sched_class->check_preempt_curr(&irmos_rq->rq_, 
+								p, flags);
+		} else {
+			for_each_class(class) {
+				if (class == curr->sched_class)
+					break;
+				if (class == p->sched_class) {
+					resched_task(curr);
+					break;
+				}
+			}
+		}
+		
+		return 0;
+	}
+
+	return irmos_rq_before(irmos_rq, irmos_rq_curr);
+}
+
+static struct irmos_rq*
+pick_next_irmos_rq(struct rq *rq)
+{
+	struct rb_node *left = rq->irmos.rb_leftmost;	
+	struct irmos_rq *irmos_rq;
+	if( !left) {
+		
+		return NULL;
+	}
+	
+	irmos_rq = rb_entry(left, struct irmos_rq, rb_node);
+	irmos_rq->irmos_start_time = rq->clock_task;
+	return irmos_rq; //rb_entry(left, struct irmos_rq, rb_node);
+}
+
+static inline bool
+irmos_rq_needs_recharge(struct irmos_rq *irmos_rq)
+{
+	if( irmos_rq->irmos_time)
+		return 1;
+
+	return 0;
+}
+
+static inline void
+sched_irmos_deadline_updated(struct irmos_rq *irmos_rq)
+{
+	int was_leftmost = irmos_rq_is_leftmost(irmos_rq);
+	
+	sched_irmos_rq_dequeue(irmos_rq);
+	sched_irmos_rq_enqueue(irmos_rq);
+
+	if( was_leftmost || irmos_rq_is_leftmost(irmos_rq))
+		resched_task(irmos_rq->rq->curr);
+}
+	
+static bool
+irmos_rq_recharge(struct irmos_rq *irmos_rq, int overrun)
+{
+	struct rq *rq = rq_of_irmos_rq(irmos_rq);
+	u64 runtime;
+	bool idle = true;
+
+	if( irmos_rq->irmos_time || irmos_rq->irmos_throttled) {
+		runtime = irmos_rq->irmos_runtime;
+		irmos_rq->irmos_time -= min(irmos_rq->irmos_time, 
+						overrun * runtime);
+		irmos_rq->irmos_deadline += overrun *
+					ktime_to_ns(irmos_rq->irmos_period);
+
+		if( irmos_rq->irmos_time || irmos_rq->irmos_nr_running)
+			idle =  false;
+
+		if( irmos_rq->irmos_throttled && 
+					irmos_rq->irmos_time < runtime) {
+			irmos_rq->irmos_throttled = 0;
+			sched_irmos_rq_enqueue(irmos_rq);
+
+			/*
+			 * Force a clock update if the cpu wad idle.
+			 * 
+			 */
+			if( irmos_rq->irmos_nr_running && rq->curr == rq->idle)
+				rq->skip_clock_update = -1;	
+		}
+		else if(!irmos_rq_throttled(irmos_rq)) {
+			sched_irmos_deadline_updated(irmos_rq);
+		}
+	}
+	else if( irmos_rq->irmos_nr_running)
+		idle = false;
+	return idle && ! irmos_rq_needs_recharge(irmos_rq);
+}
+		
+static bool
+do_sched_irmos_period_timer(struct irmos_rq *irmos_rq, int overruns)
+{
+        struct rq *rq = rq_of_irmos_rq(irmos_rq);
+        bool idle;
+
+        if( !rt_bandwidth_enabled() ||irmos_rq->irmos_runtime == RUNTIME_INF)
+                return true;
+
+        raw_spin_lock(&rq->lock);
+        raw_spin_lock(&irmos_rq->irmos_runtime_lock);
+
+        idle = irmos_rq_recharge(irmos_rq, overruns);
+
+        raw_spin_unlock(&irmos_rq->irmos_runtime_lock);
+        raw_spin_unlock(&rq->lock);
+
+        return idle;
+}
+
+static inline void
+irmos_period_set_expires(struct irmos_rq *irmos_rq, bool force)
+{
+        struct rq *rq = rq_of_irmos_rq(irmos_rq);
+        ktime_t now, dline, delta;
+
+        if(hrtimer_active(&irmos_rq->irmos_period_timer) && !force)
+                return;
+
+        /*
+         * Compensate for discrepancies between rq->clock (used to
+         * calculate deadlines) and the hrtimer-measured time, to obtain
+         * a better absolute time instant for the timer itself.
+         */
+        now = hrtimer_cb_get_time(&irmos_rq->irmos_period_timer);
+        delta = ktime_sub_ns(now, rq->clock);
+        dline = ktime_add_ns(delta, irmos_rq->irmos_deadline);
+        hrtimer_set_expires(&irmos_rq->irmos_period_timer, dline);
+}
+
+static enum hrtimer_restart
+sched_irmos_period_timer(struct hrtimer *hrtimer)
+{
+        struct irmos_rq *irmos_rq = container_of(hrtimer, struct irmos_rq,
+                                                        irmos_period_timer);
+        int overrun;
+        bool idle = false;
+
+        if( irmos_rq->irmos_needs_resync) {
+                /*
+                 * Resync the period timer with the actual deadline;
+                 * we want to be careful and check again for overruns
+                 * and recharges.
+                 * Note that if irmos_needs_resync is set we do ot need
+                 * to recharge according to the old deadline, as the 
+                 * deadline update path has rset deadline and irmos_time
+                 * for us. In the common case we do not expect overruns,
+                 * as the new deadline should be in the future, so we 
+                 * need to restart the timer only if irmos_rq is already
+                 * throttled; if this is not the case we handle the overruns
+                 * as usual, and they'll give us a new value for idle.
+                 */
+                idle = !irmos_rq->irmos_throttled;
+                irmos_rq->irmos_needs_resync = false;
+                irmos_period_set_expires(irmos_rq, true);
+        }
+
+        for(;;) {
+                overrun = hrtimer_forward_now(hrtimer, irmos_rq->irmos_period);
+                if( overrun) {
+                        /*
+                         * Recharge as if we had expired overrun times. 
+                        */
+                        idle = do_sched_irmos_period_timer(irmos_rq, overrun);
+                }
+                else {
+                        /*
+                         * No overruns, even after the eventual resync,
+                         * the timer is either ready or won't need to be
+                         * restarted.
+                         */
+                        break;
+                }
+        }
+
+        BUG_ON(irmos_rq->irmos_nr_running && !irmos_rq_on_rq(irmos_rq) && idle);
+        return idle ? HRTIMER_NORESTART : HRTIMER_RESTART;
+
+}
+
+static inline void irmos_compensate_overruns(struct irmos_rq *irmos_rq,
+							int overrun)
+{
+	if( unlikely(overrun))
+		irmos_rq_recharge(irmos_rq, overrun);
+}
+
+static void start_irmos_period_timer(struct irmos_rq *irmos_rq)
+{
+        ktime_t soft, hard;
+        unsigned long range;
+        int overrun;
+
+        irmos_period_set_expires(irmos_rq, false);
+
+        for (;;) {
+                /* Timer started, we'll get our recharge. */
+                if (hrtimer_active(&irmos_rq->irmos_period_timer))
+                        break;
+
+                /* Make sure dline is in the future when the timer starts. */
+                overrun = hrtimer_forward_now(&irmos_rq->irmos_period_timer,
+                                              irmos_rq->irmos_period);
+
+		//printk("start_irmos_period_timer: overrun=%d\n", overrun);
+                /* Update deadline and handle recharges in case of overrun. */
+                irmos_compensate_overruns(irmos_rq, overrun);
+
+                /* Avoid unnecessary timer expirations. */
+                if (!irmos_rq_needs_recharge(irmos_rq))
+                        break;
+
+
+                /* Try to program the timer. */
+                soft = hrtimer_get_softexpires(&irmos_rq->irmos_period_timer);
+                hard = hrtimer_get_expires(&irmos_rq->irmos_period_timer);
+                range = ktime_to_ns(ktime_sub(hard, soft));
+                __hrtimer_start_range_ns(&irmos_rq->irmos_period_timer, soft,
+                                         range, HRTIMER_MODE_ABS, 0);
+        }
+
+        BUG_ON(!hrtimer_active(&irmos_rq->irmos_period_timer) &&
+                irmos_rq->irmos_nr_running && !irmos_rq_on_rq(irmos_rq));
+
+}
+
+static void
+enqueue_task_irmos(struct rq *rq, struct task_struct *p, int flags)
+{
+	struct irmos_rq *irmos_rq;
+	int boosted;
+	irmos_rq = irmos_rq_of_task(p);
+	boosted = irmos_rq_boosted(irmos_rq);
+
+	update_rq_clock(rq);
+	printk("enqueue_task_irmos: pid=%d, runtime=%llu\n", p->pid, irmos_rq->irmos_runtime);
+	
+	/* 
+	 * Update the clock of local runqueue.
+	 */
+	update_rq_clock(&irmos_rq->rq_);
+	/*
+	 * Enqueue the task into the local runqueue by
+	 * the scheduling class of the task.
+	 */
+	p->sched_class->enqueue_task(&irmos_rq->rq_, p, flags);
+
+	inc_irmos_tasks(p, irmos_rq);
+	enqueue_irmos_rq(irmos_rq, boosted);
+	inc_nr_running(rq);
+}
+	
+static void
+dequeue_task_irmos(struct rq *rq, struct task_struct *p, int flags)
+{
+	struct irmos_rq *irmos_rq = irmos_rq_of_task(p);
+	int boosted = irmos_rq_boosted(irmos_rq);
+
+	update_rq_clock(rq);
+	printk("dequeue_task_irmos: pid=%d, runtime=%llu\n", p->pid, irmos_rq->irmos_runtime);
+
+	/* Will this be the solution for the biggest bug? */
+	//update_curr_irmos(rq);
+
+	update_rq_clock(&irmos_rq->rq_);
+	p->sched_class->dequeue_task(&irmos_rq->rq_, p, flags);
+
+
+	dec_irmos_tasks(p, irmos_rq);
+	dequeue_irmos_rq(irmos_rq, boosted);
+
+	dec_nr_running(rq);
+}
+
+static void
+yield_task_irmos(struct rq *rq)
+{
+	struct irmos_rq *irmos_rq = irmos_rq_of_task(rq->curr);
+	//update_curr_irmos(rq);
+
+	if( irmos_rq)
+		rq->curr->sched_class->yield_task(&irmos_rq->rq_);
+}
+
+static void 
+check_preempt_curr_irmos(struct rq *rq, struct task_struct *p, int flags)
+{
+	update_rq_clock(rq);
+	if(check_preempt_irmos_rq(rq->curr, p, flags)) {
+		resched_task(rq->curr);
+	}
+	return;
+}
+
+static struct task_struct*
+pick_next_task_irmos(struct rq *rq)
+{
+
+	struct irmos_rq *irmos_rq;
+	struct irmos_rq *prev_irmos_rq = NULL;
+	struct rq *rq_;
+	struct task_struct *p;
+	const struct sched_class *class;
+
+	update_rq_clock(rq);
+	irmos_rq = pick_next_irmos_rq(rq);
+
+	if( !irmos_rq) {
+		return NULL;
+	}
+
+	BUG_ON(!irmos_rq->irmos_nr_running);
+
+	rq_ = &irmos_rq->rq_;
+	update_rq_clock(rq_);
+
+	for_each_class(class) {
+		if( class != &idle_sched_class) {
+			p = class->pick_next_task(rq_);
+			if(p) {
+				rq_->curr = p;
+				return p;
+			}
+		}
+	}
+
+	return NULL;
+}
+	
+static void
+put_prev_task_irmos(struct rq *rq, struct task_struct *p)
+{
+	struct irmos_rq *irmos_rq = irmos_rq_of_task(p);
+	
+	update_rq_clock(rq);
+	if( p->on_rq)
+		update_rq_clock(&irmos_rq->rq_);
+
+	update_curr_irmos(rq);
+
+	p->sched_class->put_prev_task(&irmos_rq->rq_, p);
+	
+}
+
+static void
+set_curr_task_irmos(struct rq *rq)
+{
+	struct task_struct *p = rq->curr;
+	struct irmos_rq *irmos_rq = irmos_rq_of_task(p);
+
+	update_rq_clock(rq);
+	irmos_rq->irmos_start_time = rq->clock_task;
+
+	update_rq_clock(&irmos_rq->rq_);
+	p->sched_class->set_curr_task(&irmos_rq->rq_);
+}
+
+static void
+task_tick_irmos(struct rq *rq, struct task_struct *p, int queued)
+{
+	struct irmos_rq *irmos_rq = irmos_rq_of_task(p);
+
+	update_rq_clock(rq);
+	update_curr_irmos(rq);
+	
+	update_rq_clock(&irmos_rq->rq_);
+	p->sched_class->task_tick(&irmos_rq->rq_, p, queued);
+}
+
+static void
+switched_from_irmos(struct rq *rq, struct task_struct *p)
+{
+
+}
+
+static void
+switched_to_irmos(struct rq *rq, struct task_struct *p)
+{
+	struct irmos_rq *irmos_rq = irmos_rq_of_task(p);
+
+        p->sched_class->switched_to(&irmos_rq->rq_, p);
+}
+
+static void
+prio_changed_irmos(struct rq *rq, struct task_struct *p, int old_prio)
+{
+	struct irmos_rq *irmos_rq = irmos_rq_of_task(p);
+	
+	p->sched_class->prio_changed(&irmos_rq->rq_, p, old_prio);
+}
+
+static unsigned int
+get_rr_interval_irmos(struct rq *rq, struct task_struct *task)
+{
+	struct irmos_rq *irmos_rq = irmos_rq_of_task(task);
+	return task->sched_class->get_rr_interval(&irmos_rq->rq_, task);
+}
+
+static bool 
+yield_to_task_irmos(struct rq *rq, struct task_struct *p, bool preempt)
+{
+	struct task_struct *task = rq->curr;
+	struct irmos_rq *irmos_rq = irmos_rq_of_task(task);
+	
+	return p->sched_class->yield_to_task(&irmos_rq->rq_, p, preempt);
+}	
+
+const struct sched_class irmos_sched_class = {
+        //.next                   = &rt_sched_class,
+        .enqueue_task           = enqueue_task_irmos,
+        .dequeue_task           = dequeue_task_irmos,
+        .yield_task             = yield_task_irmos,
+        .yield_to_task          = yield_to_task_irmos,
+
+        .check_preempt_curr     = check_preempt_curr_irmos,
+
+        .pick_next_task         = pick_next_task_irmos,
+        .put_prev_task          = put_prev_task_irmos,
+
+	.set_curr_task          = set_curr_task_irmos,
+        .task_tick              = task_tick_irmos,
+
+        .prio_changed           = prio_changed_irmos,
+        .switched_from          = switched_from_irmos,
+        .switched_to            = switched_to_irmos,
+
+        .get_rr_interval        = get_rr_interval_irmos,
+};
